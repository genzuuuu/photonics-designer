{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([1])\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import sax\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from functools import partial\n",
    "from pprint import pprint\n",
    "\n",
    "import gdsfactory as gf\n",
    "import jax\n",
    "import jax.example_libraries.optimizers as opt\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sax\n",
    "from gdsfactory.generic_tech import get_generic_pdk\n",
    "from numpy.fft import fft2, fftfreq, fftshift, ifft2\n",
    "from rich.logging import RichHandler\n",
    "from scipy import constants\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import gplugins.sax as gs\n",
    "import gplugins.tidy3d as gt\n",
    "from gplugins.common.config import PATH\n",
    "\n",
    "gf.config.rich_output()\n",
    "PDK = get_generic_pdk()\n",
    "PDK.activate()\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.removeHandler(sys.stderr)\n",
    "logging.basicConfig(level=\"WARNING\", datefmt=\"[%X]\", handlers=[RichHandler()])\n",
    "\n",
    "gf.config.set_plot_options(show_subports=False)\n",
    "\n",
    "from math import e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global circuit_parameter_input, punishment_parameter_1, punishment_parameter_2, loss_function, s_parameter_extraction\n",
    "parameter_array_norm = np.zeros(3)\n",
    "parameter_min_array = [19.5, 1, 0]\n",
    "parameter_max_array = [20.5, 5, 4]\n",
    "circuit_parameter_input = (parameter_array_norm, parameter_min_array, parameter_max_array)\n",
    "punishment_parameter_1 = 1\n",
    "punishment_parameter_2 = 1 \n",
    "\n",
    "def loss_function(S):\n",
    "    loss= abs(S[\"o1\", \"o2\"]) ** 2\n",
    "    return loss\n",
    "\n",
    "def s_parameter_extraction(parameter_array): # This part will change\n",
    "    \n",
    "    def straight(wl=1.5, length=10.0, neff=2.4) -> sax.SDict:\n",
    "        return sax.reciprocal({(\"o1\", \"o2\"): jnp.exp(2j * jnp.pi * neff * length / wl)})\n",
    "\n",
    "    def mmi1x2():\n",
    "        \"\"\"Assumes a perfect 1x2 splitter\"\"\"\n",
    "        return sax.reciprocal(\n",
    "            {\n",
    "                (\"o1\", \"o2\"): 0.5**0.5,\n",
    "                (\"o1\", \"o3\"): 0.5**0.5,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def bend_euler(wl=1.5, length=20.0):\n",
    "        \"\"\" \"Let's assume a reduced transmission for the euler bend compared to a straight\"\"\"\n",
    "        return {k: 0.98 * v for k, v in straight(wl=wl, length=length).items()}\n",
    "\n",
    "    def waveguide(wl=1.55, wl0=1.55, neff=2.34, ng=3.4, length=10.0, loss=0.0) -> sax.SDict:\n",
    "        dwl = wl - wl0\n",
    "        dneff_dwl = (ng - neff) / wl0\n",
    "        neff = neff - dwl * dneff_dwl\n",
    "        phase = 2 * jnp.pi * neff * length / wl\n",
    "        transmission = 10 ** (-loss * length / 20) * jnp.exp(1j * phase)\n",
    "        return sax.reciprocal(\n",
    "            {\n",
    "                (\"o1\", \"o2\"): transmission,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    models = {\n",
    "        \"bend_euler\": bend_euler,\n",
    "        \"mmi1x2\": mmi1x2,\n",
    "        \"straight\": waveguide,\n",
    "    }\n",
    "\n",
    "    mzi_component = gf.components.mzi(\n",
    "        delta_length=parameter_array[0], length_x=parameter_array[2], length_y=parameter_array[1],\n",
    "    )\n",
    "        \n",
    "    mzi_circuit, _ = sax.circuit(\n",
    "        netlist=mzi_component.get_netlist(),\n",
    "        models=models,\n",
    "    )\n",
    "\n",
    "    S = mzi_circuit(\n",
    "        wl=1.53,\n",
    "        syl={\n",
    "            \"length\": parameter_array[0] / 2 + 2,\n",
    "        },\n",
    "        straight_9={\n",
    "            \"length\": parameter_array[0] / 2 + 2,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class circuit_env(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(circuit_env, self).__init__()\n",
    "\n",
    "        self.parameter_array_norm = circuit_parameter_input[0]\n",
    "        self.parameter_min_array = circuit_parameter_input[1]\n",
    "        self.parameter_max_array = circuit_parameter_input[2]\n",
    "\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1, high=1, shape=(len(self.parameter_array_norm),), dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-1, high=1, shape=(len(self.parameter_array_norm),), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.parameter_array_norm = np.zeros(3)\n",
    "        return np.array(self.parameter_array_norm).astype(np.float32), {} \n",
    "\n",
    "    def step(self, action):\n",
    "        i=0\n",
    "        punishment_total = 0\n",
    "        parameter_array = self.reverse_normalization()\n",
    "        S = s_parameter_extraction(parameter_array)\n",
    "        while i < len(action):\n",
    "            if ((action[i] <= 0) & (self.parameter_array_norm[i]+action[i]>-1)) | ((action[i] >= 0) & (self.parameter_array_norm[i]+action[i]<1)):\n",
    "                self.parameter_array_norm[i] += float(action[i])\n",
    "                punishment_total = punishment_total\n",
    "            else:\n",
    "                punishment_total = punishment_total + punishment_parameter_1\n",
    "            i=i+1\n",
    "        punishment_total = punishment_total + sum(abs(action))*punishment_parameter_2\n",
    "        reward = float(1-loss_function(S)-punishment_total)\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False  # we do not limit the number of steps here\n",
    "        info = {}\n",
    "        return (\n",
    "            np.array(self.parameter_array_norm).astype(np.float32),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def reverse_normalization(self):\n",
    "        parameter_array = np.zeros(len(self.parameter_array_norm))\n",
    "        i = 0 \n",
    "        while i < len(self.parameter_array_norm):\n",
    "            parameter_array[i] = self.parameter_min_array[i] + (self.parameter_max_array[i] - self.parameter_min_array[i]) * (self.parameter_array_norm[i]+1)/2\n",
    "            i = i + 1\n",
    "        return parameter_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "env = circuit_env()\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)\n",
    "\n",
    "from stable_baselines3 import PPO, A2C, DQN, DDPG\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "vec_env = make_vec_env(circuit_env, n_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=0, n_steps=20, gamma=0.95).learn(4000, progress_bar=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent\n",
    "# using the vecenv\n",
    "\n",
    "\n",
    "obs = vec_env.reset()\n",
    "obs_array = []\n",
    "reward_array = []\n",
    "action_array = []\n",
    "n_steps = 20\n",
    "steps_array = np.linspace(1,n_steps,n_steps)\n",
    "for step in steps_array:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    #print(f\"Step {step + 1}\")\n",
    "    # print(\"Action: \", action)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    # print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    vec_env.render()\n",
    "    obs_array = obs_array + [obs]\n",
    "    reward_array = reward_array + [reward]\n",
    "    action_array = action_array + [action*0.5]\n",
    "    # if reward>1.95:\n",
    "    #     # Note that the VecEnv resets automatically\n",
    "    #     # when a done signal is encountered\n",
    "    #     print(\"Goal reached!\", \"reward=\", reward)\n",
    "    #     break\n",
    "\n",
    "display(\"Final results: delta_length = \" + str(obs[0][0]*0.5+20) + \", length_y = \" + str(obs[0][1]*2+3) + \", length_x = \" + str(obs[0][2]*2+2) +  \", reward = \"+str(reward) )\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(reward_array)\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.grid()\n",
    "\n",
    "obs_array_ = np.array(obs_array)\n",
    "action_array_ = np.array(action_array)\n",
    "\n",
    "plt.figure()\n",
    "line1,=plt.plot(obs_array_[:,:,0]*0.5+20, label=\"delta_length\")\n",
    "line2,=plt.plot(obs_array_[:,:,1]*2+3, label=\"length_y\")\n",
    "line3,=plt.plot(obs_array_[:,:,2]*2+2, label=(\"length_x\"))\n",
    "plt.xlabel(\"steps\")\n",
    "plt.legend(handles=[line1, line2, line3])\n",
    "plt.grid()\n",
    "\n",
    "plt.figure()\n",
    "line1,=plt.plot(action_array_[:,:,0]*0.5, label=\"delta_length-action\")\n",
    "line2,=plt.plot(action_array_[:,:,1]*2, label=\"length_y-action\")\n",
    "line3,=plt.plot(action_array_[:,:,2]*2, label=\"length_x-action\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.legend(handles=[line1, line2, line3])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "model_2 = PPO(\"MlpPolicy\", env, verbose=0, n_steps=10, gamma=0.90).learn(500, progress_bar=True)\n",
    "# Test the trained agent\n",
    "# using the vecenv\n",
    "\n",
    "\n",
    "obs = vec_env.reset()\n",
    "obs_array = []\n",
    "reward_array = []\n",
    "action_array = []\n",
    "n_steps = 50\n",
    "steps_array = np.linspace(1,n_steps,n_steps)\n",
    "for step in steps_array:\n",
    "    action, _ = model_2.predict(obs, deterministic=True)\n",
    "    #print(f\"Step {step + 1}\")\n",
    "    # print(\"Action: \", action)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    # print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    vec_env.render()\n",
    "    obs_array = obs_array + [obs]\n",
    "    reward_array = reward_array + [reward]\n",
    "    action_array = action_array + [action*0.5]\n",
    "    # if reward>1.95:\n",
    "    #     # Note that the VecEnv resets automatically\n",
    "    #     # when a done signal is encountered\n",
    "    #     print(\"Goal reached!\", \"reward=\", reward)\n",
    "    #     break\n",
    "\n",
    "display(\"Final results: delta_length = \" + str(obs[0][0]*0.5+20) + \", length_y = \" + str(obs[0][1]*2+3) + \", length_x = \" + str(obs[0][2]*2+2) +  \", reward = \"+str(reward) )\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(reward_array)\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.grid()\n",
    "\n",
    "obs_array_ = np.array(obs_array)\n",
    "action_array_ = np.array(action_array)\n",
    "\n",
    "plt.figure()\n",
    "line1,=plt.plot(obs_array_[:,:,0]*0.5+20, label=\"delta_length\")\n",
    "line2,=plt.plot(obs_array_[:,:,1]*2+3, label=\"length_y\")\n",
    "line3,=plt.plot(obs_array_[:,:,2]*2+2, label=(\"length_x\"))\n",
    "plt.xlabel(\"steps\")\n",
    "plt.legend(handles=[line1, line2, line3])\n",
    "plt.grid()\n",
    "\n",
    "plt.figure()\n",
    "line1,=plt.plot(action_array_[:,:,0]*0.5, label=\"delta_length-action\")\n",
    "line2,=plt.plot(action_array_[:,:,1]*2, label=\"length_y-action\")\n",
    "line3,=plt.plot(action_array_[:,:,2]*2, label=\"length_x-action\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.legend(handles=[line1, line2, line3])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = vec_env.reset()\n",
    "obs_array = []\n",
    "reward_array = []\n",
    "action_array = []\n",
    "n_steps = 40\n",
    "steps_array = np.linspace(1,n_steps,n_steps)\n",
    "for step in steps_array:\n",
    "    action, _ = model_2.predict(obs, deterministic=True)\n",
    "    #print(f\"Step {step + 1}\")\n",
    "    # print(\"Action: \", action)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    # print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    vec_env.render()\n",
    "    obs_array = obs_array + [obs]\n",
    "    reward_array = reward_array + [reward]\n",
    "    action_array = action_array + [action*0.5]\n",
    "    # if reward>1.95:\n",
    "    #     # Note that the VecEnv resets automatically\n",
    "    #     # when a done signal is encountered\n",
    "    #     print(\"Goal reached!\", \"reward=\", reward)\n",
    "    #     break\n",
    "\n",
    "display(\"Final results: delta_length = \" + str(obs[0][0]*0.5+20) + \", length_y = \" + str(obs[0][1]*2+3) + \", length_x = \" + str(obs[0][2]*2+2) +  \", reward = \"+str(reward) )\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(reward_array)\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.grid()\n",
    "\n",
    "obs_array_ = np.array(obs_array)\n",
    "action_array_ = np.array(action_array)\n",
    "\n",
    "plt.figure()\n",
    "line1,=plt.plot(obs_array_[:,:,0]*0.5+20, label=\"delta_length\")\n",
    "line2,=plt.plot(obs_array_[:,:,1]*2+3, label=\"length_y\")\n",
    "line3,=plt.plot(obs_array_[:,:,2]*2+2, label=(\"length_x\"))\n",
    "plt.xlabel(\"steps\")\n",
    "plt.legend(handles=[line1, line2, line3])\n",
    "plt.grid()\n",
    "\n",
    "plt.figure()\n",
    "line1,=plt.plot(action_array_[:,:,0]*0.5, label=\"delta_length-action\")\n",
    "line2,=plt.plot(action_array_[:,:,1]*2, label=\"length_y-action\")\n",
    "line3,=plt.plot(action_array_[:,:,2]*2, label=\"length_x-action\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.legend(handles=[line1, line2, line3])\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
